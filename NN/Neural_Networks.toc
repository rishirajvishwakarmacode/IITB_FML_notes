\contentsline {part}{I\hspace {1em}Neural Networks}{4}{part.1}%
\contentsline {chapter}{\numberline {1}Feed Forward Neural Networks}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}Feed-forward Network Functions}{5}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}A Series of Functional Transformations}{5}{subsection.1.1.1}%
\contentsline {subsubsection}{Step 1: First Layer (Input to Hidden)}{5}{section*.2}%
\contentsline {subsubsection}{Step 2: Hidden Unit Activation Function}{5}{section*.3}%
\contentsline {subsubsection}{Step 3: Second Layer (Hidden to Output)}{6}{section*.4}%
\contentsline {subsubsection}{Step 4: Output Unit Activation Function}{6}{section*.5}%
\contentsline {subsection}{\numberline {1.1.2}Overall Network Function}{6}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Differentiability of the Network Function}{7}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Important Results and Conditions}{8}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Universal Approximation Theorem}{8}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Feed-Forward Architecture}{9}{subsection.1.2.2}%
\contentsline {section}{\numberline {1.3}Weight-space Symmetries}{9}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Baseline Network (Network A)}{9}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Symmetry 1: Sign-Flip}{10}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Symmetry 2: Interchange}{10}{subsection.1.3.3}%
\contentsline {chapter}{\numberline {2}Network training}{11}{chapter.2}%
\contentsline {section}{\numberline {2.1}Fundamentals}{11}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Regression Problems}{11}{subsection.2.1.1}%
\contentsline {subsubsection}{The Error Function (Maximum Likelihood)}{11}{section*.6}%
\contentsline {subsubsection}{Determining the Noise Precision $\beta $}{12}{section*.7}%
\contentsline {subsubsection}{Multiple Target Variables (K outputs)}{12}{section*.8}%
\contentsline {subsubsection}{Derivative of the Error Function}{12}{section*.9}%
\contentsline {subsection}{\numberline {2.1.2}Classification Problems}{13}{subsection.2.1.2}%
\contentsline {subsubsection}{Binary Classification}{13}{section*.10}%
\contentsline {subsubsection}{Multiple Independent Binary Classifications}{14}{section*.11}%
\contentsline {subsubsection}{Derivative of the Error Function}{14}{section*.12}%
\contentsline {subsection}{\numberline {2.1.3}Numerical Example: Binary Classification}{15}{subsection.2.1.3}%
\contentsline {subsubsection}{Parameters and Data}{15}{section*.13}%
\contentsline {subsubsection}{Step 1: Forward Propagation}{15}{section*.14}%
\contentsline {subsubsection}{Step 2: Calculate the Error}{16}{section*.15}%
\contentsline {section}{\numberline {2.2}Parameter Optimisation}{16}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Challenges: Non-Convexity and Multiple Minima}{16}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Impossibility of an Analytic Solution}{16}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Iterative Numerical Solution}{16}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Local Quadratic Approximation \(Optional\)}{16}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Primer on the Mathematics}{17}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}The Quadratic Approximation}{17}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Analysis at a Minimum}{17}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Significance in Neural Network Training}{18}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Gradient Descent Optimization}{18}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Batch Gradient Descent}{18}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}On-line (Stochastic) Gradient Descent}{18}{subsection.2.4.2}%
\contentsline {chapter}{\numberline {3}Error Backpropagation}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Backpropagation Example: A 1-1-1-1 Network}{19}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}1. Network \& Problem Setup}{19}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}2. The Forward Propagation Pass}{19}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}3. The Backward Propagation Pass (The Derivatives)}{19}{subsection.3.1.3}%
\contentsline {subsubsection}{Step 3a: Gradients for Layer 3 (Output Layer)}{20}{section*.16}%
\contentsline {subsubsection}{Step 3b: Gradients for Layer 2 (Hidden Layer H2)}{20}{section*.17}%
\contentsline {subsubsection}{Step 3c: Gradients for Layer 1 (Hidden Layer H1)}{21}{section*.18}%
\contentsline {subsection}{\numberline {3.1.4}4. Summary of the Algorithm}{21}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Backpropagation Example: A 2-3-2-1 Network}{22}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Network \& Problem Setup}{22}{subsection.3.2.1}%
\contentsline {subsubsection}{Network Diagram}{22}{section*.19}%
\contentsline {subsection}{\numberline {3.2.2}1. Forward Propagation}{22}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}2. Backward Propagation (The Derivatives)}{22}{subsection.3.2.3}%
\contentsline {subsubsection}{Step 2a: Gradients for Layer 3 (Output)}{22}{section*.20}%
\contentsline {subsubsection}{Step 2b: Gradients for Layer 2 (H2)}{23}{section*.21}%
\contentsline {subsubsection}{Step 2c: Gradients for Layer 1 (H1)}{24}{section*.22}%
\contentsline {subsection}{\numberline {3.2.4}3. Summary of the Algorithm}{24}{subsection.3.2.4}%
\contentsline {section}{\numberline {3.3}The General Rules of Backpropagation}{25}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Rule 1: Calculating the $\delta $ for an Output Unit}{25}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Rule 2: Calculating the $\delta $ for a Hidden Unit}{25}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Rule 3: The Gradient Calculation Rule}{26}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Summary of the Backpropagation Algorithm}{26}{subsection.3.3.4}%
\contentsline {section}{\numberline {3.4}Backpropagation Practice Questions}{26}{section.3.4}%
\contentsline {chapter}{\numberline {4}Regularization In Neural Networks}{29}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{29}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Weight Decay}{29}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}Consistent Gaussian Priors (and limitations of Weight Decay)}{30}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Invariance to Input Transformations}{30}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Invariance to Output Transformations}{31}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Weight Decay lacks Invariance}{32}{subsection.4.2.3}%
\contentsline {section}{\numberline {4.3}The Right One.......}{33}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Logical Deduction of a Consistent Regularizer}{33}{subsection.4.3.1}%
\contentsline {subsubsection}{Problem 1: The Bias Invariance}{33}{section*.23}%
\contentsline {subsubsection}{Problem 2: The Weight Invariance}{33}{section*.24}%
\contentsline {subsection}{\numberline {4.3.2}The Generalised Regularizer}{33}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Early Stopping}{34}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Definition}{34}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Significance as a Regularizer}{34}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mathematical Justification (Proof of Equivalence)}{34}{subsection.4.4.3}%
\contentsline {subsubsection}{Setup}{34}{section*.25}%
\contentsline {subsubsection}{Analysis of Gradient Descent Update}{34}{section*.26}%
\contentsline {subsubsection}{Analysis of Weight Decay Solution}{35}{section*.27}%
\contentsline {subsection}{\numberline {4.4.4}Conclusion: A Comparison}{35}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Invariances in Neural Networks}{35}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Problem}{36}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Four Approaches to Achieve Invariance}{36}{subsection.4.5.2}%
\contentsline {subsubsection}{1. Augmenting the Training Set}{36}{section*.28}%
\contentsline {subsubsection}{2. Regularization}{36}{section*.29}%
\contentsline {subsubsection}{3. Invariant Feature Pre-processing}{36}{section*.30}%
\contentsline {subsubsection}{4. Building Invariance into the Architecture}{36}{section*.31}%
\contentsline {chapter}{\numberline {5}Implementing Logical Functions with Neural Networks}{37}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduction: The Perceptron Model}{37}{section.5.1}%
\contentsline {paragraph}{Linear Combination}{37}{section*.32}%
\contentsline {paragraph}{Activation Function}{37}{section*.33}%
\contentsline {paragraph}{Truth Table Convention}{37}{section*.34}%
\contentsline {section}{\numberline {5.2}Basic Logic Functions (Linearly Separable)}{37}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}AND Logic}{37}{subsection.5.2.1}%
\contentsline {paragraph}{Verification:}{38}{section*.35}%
\contentsline {subsection}{\numberline {5.2.2}OR Logic}{38}{subsection.5.2.2}%
\contentsline {paragraph}{Verification:}{38}{section*.36}%
\contentsline {subsection}{\numberline {5.2.3}NOT Logic}{39}{subsection.5.2.3}%
\contentsline {paragraph}{Verification:}{39}{section*.37}%
\contentsline {subsection}{\numberline {5.2.4}NAND Logic (NOT AND)}{39}{subsection.5.2.4}%
\contentsline {paragraph}{Verification:}{39}{section*.38}%
\contentsline {subsection}{\numberline {5.2.5}NOR Logic (NOT OR)}{40}{subsection.5.2.5}%
\contentsline {paragraph}{Verification:}{40}{section*.39}%
\contentsline {section}{\numberline {5.3}XOR Logic (Non-Linearly Separable)}{41}{section.5.3}%
\contentsline {paragraph}{Verification:}{41}{section*.40}%
\contentsline {chapter}{\numberline {6}Building and Generalizing Logic Functions}{42}{chapter.6}%
\contentsline {section}{\numberline {6.1}Composing Functions}{42}{section.6.1}%
\contentsline {section}{\numberline {6.2}Generalization to \emph {n} Inputs}{43}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}n-Input AND Gate}{43}{subsection.6.2.1}%
\contentsline {paragraph}{Verification:}{43}{section*.41}%
\contentsline {subsection}{\numberline {6.2.2}n-Input OR Gate}{43}{subsection.6.2.2}%
\contentsline {paragraph}{Verification:}{43}{section*.42}%
\contentsline {subsection}{\numberline {6.2.3}n-Input NAND Gate}{43}{subsection.6.2.3}%
\contentsline {paragraph}{Verification:}{43}{section*.43}%
\contentsline {subsection}{\numberline {6.2.4}n-Input NOR Gate}{43}{subsection.6.2.4}%
\contentsline {paragraph}{Verification:}{43}{section*.44}%
\contentsline {section}{\numberline {6.3}General Method for Arbitrary $n$-Input Functions}{43}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}The DNF Network Architecture}{44}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Rule for Minterm Neurons}{44}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Example: 3-Input Parity Function (3-XOR)}{44}{subsection.6.3.3}%
\contentsline {paragraph}{1. Truth Table and Minterms}{44}{section*.45}%
\contentsline {paragraph}{2. DNF (Function)}{44}{section*.46}%
\contentsline {paragraph}{3. Network Architecture (3-4-1)}{44}{section*.47}%
\contentsline {paragraph}{4. Weights and Biases}{45}{section*.48}%
\contentsline {paragraph}{5. Final Network Diagram}{45}{section*.49}%
\contentsline {part}{II\hspace {1em}Convolutional Neural Networks}{46}{part.2}%
\contentsline {chapter}{\numberline {7}Convolutional Filters}{47}{chapter.7}%
\contentsline {section}{\numberline {7.1}Introduction}{47}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Motivation and Inductive Bias}{47}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Four Key Concepts for Image Structure}{47}{subsection.7.1.2}%
\contentsline {section}{\numberline {7.2}Feature Detectors}{47}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Locality and Receptive Fields}{47}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Mathematical Definition}{47}{subsection.7.2.2}%
\contentsline {paragraph}{Filter / Kernel}{47}{section*.50}%
\contentsline {paragraph}{Vector Notation}{48}{section*.51}%
\contentsline {paragraph}{2D Convolutional Notation}{48}{section*.52}%
\contentsline {subsection}{\numberline {7.2.3}How it Works}{48}{subsection.7.2.3}%
\contentsline {part}{III\hspace {1em}Recurrent Neural Networks}{49}{part.3}%
\contentsline {chapter}{\numberline {8}Foundations of Sequence Modelling}{50}{chapter.8}%
\contentsline {section}{\numberline {8.1}The Problem: Sequential Data}{50}{section.8.1}%
\contentsline {subsection}{\numberline {8.1.1}Failure 1: Fixed-Size Input and Output}{50}{subsection.8.1.1}%
\contentsline {subsection}{\numberline {8.1.2}Failure 2: No Parameter Sharing Across Time}{50}{subsection.8.1.2}%
\contentsline {subsection}{\numberline {8.1.3}Failure 3: No "Memory" or Internal State}{51}{subsection.8.1.3}%
\contentsline {section}{\numberline {8.2}Building the RNN Equations}{51}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Case 1: The Simplest RNN (1-1-1 Network)}{51}{subsection.8.2.1}%
\contentsline {subsubsection}{Weight "Matrices" as Scalars}{51}{section*.53}%
\contentsline {subsubsection}{The Forward Pass Equations (Scalar Form)}{51}{section*.54}%
\contentsline {subsection}{\numberline {8.2.2}Case 2: 1 Input, $D_h$ Hidden Neurons (1-$D_h$-1 Network)}{52}{subsection.8.2.2}%
\contentsline {subsubsection}{Weight Matrices (from Scalars to Vectors/Matrices)}{52}{section*.55}%
\contentsline {subsubsection}{The Forward Pass Equations (Vector Form)}{52}{section*.56}%
\contentsline {subsection}{\numberline {8.2.3}Case 3: $D_x$ Inputs, $D_h$ Hidden Neurons (The General Case)}{53}{subsection.8.2.3}%
\contentsline {subsubsection}{Weight Matrices (Full Matrix Form)}{54}{section*.57}%
\contentsline {subsubsection}{The Forward Pass Equations (Final Vector Form)}{54}{section*.58}%
\contentsline {subsection}{\numberline {8.2.4}The General Case: RNN Equations}{54}{subsection.8.2.4}%
\contentsline {subsubsection}{Definitions}{54}{section*.59}%
\contentsline {subsubsection}{Shared Parameters (The "Rule")}{55}{section*.60}%
\contentsline {subsubsection}{Forward Pass Equations}{55}{section*.61}%
\contentsline {section}{\numberline {8.3}The "Unfolding in Time" Computational Graph}{56}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}Mathematical Definition of the Unfolded Graph}{56}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Diagram of the Unfolded Graph (for $T=3$)}{56}{subsection.8.3.2}%
\contentsline {chapter}{\numberline {9}Ndtwork Training}{58}{chapter.9}%
\contentsline {section}{\numberline {9.1}Defining the Loss Function}{58}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}The Per-Time-Step Loss: $E^{(t)}$}{58}{subsection.9.1.1}%
\contentsline {subsubsection}{Case 1: Regression}{58}{section*.62}%
\contentsline {subsubsection}{Case 2: Classification}{58}{section*.63}%
\contentsline {subsection}{\numberline {9.1.2}The Total Loss: $E$}{58}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Significance for Gradient Calculation}{59}{subsection.9.1.3}%
\contentsline {section}{\numberline {9.2}Module 2.2: Deriving the Gradients (BPTT)}{59}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Case 1: Gradient for Output Weights ($\mathbf {W}_{hy}$)}{59}{subsection.9.2.1}%
\contentsline {subsubsection}{Mathematical Derivation (per time step $t$)}{59}{section*.64}%
\contentsline {subsubsection}{Final Gradient Calculation}{60}{section*.65}%
\contentsline {section}{\numberline {9.3}Learn with example}{60}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Part 1: The Problem Definition}{60}{subsection.9.3.1}%
\contentsline {subsubsection}{1.1: Architecture}{60}{section*.66}%
\contentsline {subsubsection}{1.2: Mathematical Equations}{60}{section*.67}%
\contentsline {subsubsection}{1.3: The Unfolded Computational Graph (T=2)}{60}{section*.68}%
\contentsline {subsection}{\numberline {9.3.2}Part 2: The Numerical Example}{61}{subsection.9.3.2}%
\contentsline {subsubsection}{2.1: Initial Values}{61}{section*.69}%
\contentsline {subsection}{\numberline {9.3.3}Part 3: Forward Pass (Numerical Calculation)}{62}{subsection.9.3.3}%
\contentsline {subsubsection}{3.1: Time Step $t=1$}{62}{section*.70}%
\contentsline {subsubsection}{3.2: Time Step $t=2$}{62}{section*.71}%
\contentsline {subsubsection}{3.3: Total Loss}{62}{section*.72}%
\contentsline {subsection}{\numberline {9.3.4}Part 4: Backward Pass (BPTT Derivations \& Calculations)}{63}{subsection.9.3.4}%
\contentsline {subsubsection}{4.1: Gradient for Output Weights $\mathbf {W}_{hy}$ and $\mathbf {b}_y$}{63}{section*.73}%
\contentsline {paragraph}{Step 4.1.1: General Derivation (per time step t)}{63}{section*.74}%
\contentsline {paragraph}{Step 4.1.2: Numerical Calculation}{63}{section*.75}%
\contentsline {subsubsection}{4.2: Gradients for Hidden Weights $\mathbf {W}_{hh}$, $\mathbf {W}_{xh}$, $\mathbf {b}_h$}{63}{section*.76}%
\contentsline {paragraph}{Step 4.2.1: General Derivation of $\delta _h^{(t)}$}{64}{section*.77}%
\contentsline {paragraph}{Step 4.2.2: Numerical Calculation of $\delta _h^{(t)}$}{64}{section*.78}%
\contentsline {paragraph}{Step 4.2.3: Final Gradient Calculation (W, b)}{65}{section*.79}%
\contentsline {subsection}{\numberline {9.3.5}Part 5: Weight Update (Example)}{66}{subsection.9.3.5}%
\contentsline {paragraph}{Update for $\mathbf {W}_{hh}$:}{66}{section*.80}%
\contentsline {paragraph}{Update for $\mathbf {W}_{hy}$:}{66}{section*.81}%
\contentsline {section}{\numberline {9.4}The Recursive Jacobian Product}{66}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}The Jacobian of the State Transition}{66}{subsection.9.4.1}%
\contentsline {section}{\numberline {9.5}The Problem of Long-Range Dependencies}{67}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}3.1 Vanishing Gradients}{67}{subsection.9.5.1}%
\contentsline {subsection}{\numberline {9.5.2}3.2 Exploding Gradients}{68}{subsection.9.5.2}%
\contentsline {subsection}{\numberline {9.5.3}3.3 Solution for Exploding Gradients: Gradient Clipping}{68}{subsection.9.5.3}%
\contentsline {chapter}{\numberline {10}Gated RNNs}{69}{chapter.10}%
\contentsline {section}{\numberline {10.1}The Gating Mechanism}{69}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Problem: The Unstable Gradient Path}{69}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}The Conceptual Solution: An Additive "Cell State"}{69}{subsection.10.1.2}%
\contentsline {subsection}{\numberline {10.1.3}The Gating Mechanism: A Learnable "Soft Switch"}{69}{subsection.10.1.3}%
\contentsline {subsubsection}{Mathematical Definition of a Gate}{70}{section*.82}%
\contentsline {paragraph}{1. The Gate Vector $\mathbf {g}$:}{70}{section*.83}%
\contentsline {paragraph}{2. The Gating Operation $\odot $:}{70}{section*.84}%
\contentsline {paragraph}{Numerical Example:}{70}{section*.85}%
\contentsline {subsection}{\numberline {10.1.4}The Gated Gradient Superhighway}{70}{subsection.10.1.4}%
\contentsline {subsubsection}{Gradient Analysis of the Gated Highway}{70}{section*.86}%
\contentsline {part}{IV\hspace {1em}Transformers}{72}{part.4}%
\contentsline {chapter}{\numberline {11}Attention}{73}{chapter.11}%
\contentsline {section}{\numberline {11.1}Introduction to Attention}{73}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Key Takeaways from the Introduction}{73}{subsection.11.1.1}%
\contentsline {section}{\numberline {11.2}Attention Coefficients}{73}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Constraints on Attention Weights}{74}{subsection.11.2.1}%
\contentsline {chapter}{Appendix: Common Activation Functions}{75}{appendix*.87}%
\contentsline {subsubsection}{Identity Function}{75}{section*.89}%
\contentsline {subsubsection}{Logistic Sigmoid}{75}{section*.90}%
\contentsline {subsubsection}{Hyperbolic Tangent (tanh)}{75}{section*.91}%
\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{75}{section*.92}%
\contentsline {subsubsection}{Softmax}{76}{section*.94}%
