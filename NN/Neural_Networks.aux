\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Neural Networks}{4}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Feed Forward Neural Networks}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Feed-forward Network Functions}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}A Series of Functional Transformations}{5}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 1: First Layer (Input to Hidden)}{5}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 2: Hidden Unit Activation Function}{5}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 3: Second Layer (Hidden to Output)}{6}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 4: Output Unit Activation Function}{6}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Overall Network Function}{6}{subsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A 3-4-2 feed-forward network. Bias units $x_0$ and $z_0$ are included.}}{7}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Differentiability of the Network Function}{7}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Important Results and Conditions}{8}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Universal Approximation Theorem}{8}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Feed-Forward Architecture}{9}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Weight-space Symmetries}{9}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Baseline Network (Network A)}{9}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Symmetry 1: Sign-Flip}{10}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Symmetry 2: Interchange}{10}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Network training}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Fundamentals}{11}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Regression Problems}{11}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Error Function (Maximum Likelihood)}{11}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Determining the Noise Precision $\beta $}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multiple Target Variables (K outputs)}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Derivative of the Error Function}{12}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Classification Problems}{13}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Binary Classification}{13}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multiple Independent Binary Classifications}{14}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Derivative of the Error Function}{14}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Numerical Example: Binary Classification}{15}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Parameters and Data}{15}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 1: Forward Propagation}{15}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 2: Calculate the Error}{16}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Parameter Optimisation}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Challenges: Non-Convexity and Multiple Minima}{16}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Impossibility of an Analytic Solution}{16}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Iterative Numerical Solution}{16}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Local Quadratic Approximation \(Optional\)}{16}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Primer on the Mathematics}{17}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The Quadratic Approximation}{17}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Analysis at a Minimum}{17}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Significance in Neural Network Training}{18}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Gradient Descent Optimization}{18}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Batch Gradient Descent}{18}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}On-line (Stochastic) Gradient Descent}{18}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Error Backpropagation}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Backpropagation Example: A 1-1-1-1 Network}{19}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}1. Network \& Problem Setup}{19}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}2. The Forward Propagation Pass}{19}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}3. The Backward Propagation Pass (The Derivatives)}{19}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 3a: Gradients for Layer 3 (Output Layer)}{20}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 3b: Gradients for Layer 2 (Hidden Layer H2)}{20}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 3c: Gradients for Layer 1 (Hidden Layer H1)}{21}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}4. Summary of the Algorithm}{21}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation Example: A 2-3-2-1 Network}{22}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Network \& Problem Setup}{22}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Network Diagram}{22}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}1. Forward Propagation}{22}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}2. Backward Propagation (The Derivatives)}{22}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 2a: Gradients for Layer 3 (Output)}{22}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A 2-3-2-1 feed-forward network with bias units for each layer.}}{23}{figure.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 2b: Gradients for Layer 2 (H2)}{23}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step 2c: Gradients for Layer 1 (H1)}{24}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}3. Summary of the Algorithm}{24}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The General Rules of Backpropagation}{25}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Rule 1: Calculating the $\delta $ for an Output Unit}{25}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Rule 1: The error signal $\delta _k$ for an output unit is the derivative of the final error $E_n$ with respect to the unit's own activation $a_k$.}}{25}{figure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Rule 2: Calculating the $\delta $ for a Hidden Unit}{25}{subsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Rule 2: The error $\delta _j$ is the \textbf  {sum} of all incoming error signals from the next layer ($\DOTSB \sum@ \slimits@ _k w_{kj} \delta _k$), multiplied by the local derivative $h'(a_j)$.}}{26}{figure.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Rule 3: The Gradient Calculation Rule}{26}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Rule 3: The gradient for the weight $w_{ji}$ is the product of the output from the source unit ($z_i$) and the error signal at the destination unit ($\delta _j$).}}{26}{figure.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Summary of the Backpropagation Algorithm}{26}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Backpropagation Practice Questions}{26}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Regularization In Neural Networks}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Weight Decay}{29}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Consistent Gaussian Priors (and limitations of Weight Decay)}{30}{section.4.2}\protected@file@percent }
\newlabel{eq:hidden_act_2}{{4.2}{30}{Consistent Gaussian Priors (and limitations of Weight Decay)}{equation.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Invariance to Input Transformations}{30}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Invariance to Output Transformations}{31}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Weight Decay lacks Invariance}{32}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Right One.......}{33}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Logical Deduction of a Consistent Regularizer}{33}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Problem 1: The Bias Invariance}{33}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Problem 2: The Weight Invariance}{33}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}The Generalised Regularizer}{33}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Early Stopping}{34}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Definition}{34}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Significance as a Regularizer}{34}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Mathematical Justification (Proof of Equivalence)}{34}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Setup}{34}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analysis of Gradient Descent Update}{34}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analysis of Weight Decay Solution}{35}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Conclusion: A Comparison}{35}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Invariances in Neural Networks}{35}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}The Problem}{36}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Four Approaches to Achieve Invariance}{36}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1. Augmenting the Training Set}{36}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2. Regularization}{36}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3. Invariant Feature Pre-processing}{36}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4. Building Invariance into the Architecture}{36}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementing Logical Functions with Neural Networks}{37}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction: The Perceptron Model}{37}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Combination}{37}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Activation Function}{37}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Truth Table Convention}{37}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Basic Logic Functions (Linearly Separable)}{37}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}AND Logic}{37}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{38}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}OR Logic}{38}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{38}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}NOT Logic}{39}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{39}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}NAND Logic (NOT AND)}{39}{subsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{39}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}NOR Logic (NOT OR)}{40}{subsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{40}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}XOR Logic (Non-Linearly Separable)}{41}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{41}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Building and Generalizing Logic Functions}{42}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Composing Functions}{42}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Generalization to \emph  {n} Inputs}{43}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}n-Input AND Gate}{43}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{43}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}n-Input OR Gate}{43}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{43}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}n-Input NAND Gate}{43}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{43}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}n-Input NOR Gate}{43}{subsection.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification:}{43}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}General Method for Arbitrary $n$-Input Functions}{43}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}The DNF Network Architecture}{44}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Rule for Minterm Neurons}{44}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Example: 3-Input Parity Function (3-XOR)}{44}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Truth Table and Minterms}{44}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. DNF (Function)}{44}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Network Architecture (3-4-1)}{44}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Weights and Biases}{45}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Final Network Diagram}{45}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Convolutional Neural Networks}{46}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Convolutional Filters}{47}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{47}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Motivation and Inductive Bias}{47}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Four Key Concepts for Image Structure}{47}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Feature Detectors}{47}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Locality and Receptive Fields}{47}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Mathematical Definition}{47}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Filter / Kernel}{47}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vector Notation}{48}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2D Convolutional Notation}{48}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}How it Works}{48}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Recurrent Neural Networks}{49}{part.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Foundations of Sequence Modelling}{50}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}The Problem: Sequential Data}{50}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Failure 1: Fixed-Size Input and Output}{50}{subsection.8.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Failure 2: No Parameter Sharing Across Time}{50}{subsection.8.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Failure 3: No "Memory" or Internal State}{51}{subsection.8.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Building the RNN Equations}{51}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Case 1: The Simplest RNN (1-1-1 Network)}{51}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Weight "Matrices" as Scalars}{51}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Forward Pass Equations (Scalar Form)}{51}{section*.54}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces A 1-1-1 network, showing the scalar weights for input ($w_{xh}$), hidden state ($w_{hh}$), and output ($w_{hy}$). Biases are omitted for clarity.}}{52}{figure.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Case 2: 1 Input, $D_h$ Hidden Neurons (1-$D_h$-1 Network)}{52}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Weight Matrices (from Scalars to Vectors/Matrices)}{52}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Forward Pass Equations (Vector Form)}{52}{section*.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces A 1-$D_h$-1 network. $\mathbf  {W}_{xh}$ is a vector (left), $\mathbf  {W}_{hy}$ is a vector (right), and $\mathbf  {W}_{hh}$ is a full matrix (red, dashed) connecting all $D_h$ previous states to all $D_h$ current states.}}{53}{figure.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Case 3: $D_x$ Inputs, $D_h$ Hidden Neurons (The General Case)}{53}{subsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Weight Matrices (Full Matrix Form)}{54}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Forward Pass Equations (Final Vector Form)}{54}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}The General Case: RNN Equations}{54}{subsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Definitions}{54}{section*.59}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The general RNN case ($D_x > 1, D_h > 1$). $\mathbf  {W}_{xh}$ (blue) and $\mathbf  {W}_{hh}$ (red) are now both full matrices, and their outputs are added at each hidden neuron.}}{55}{figure.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Shared Parameters (The "Rule")}{55}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass Equations}{55}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}The "Unfolding in Time" Computational Graph}{56}{section.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Mathematical Definition of the Unfolded Graph}{56}{subsection.8.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Diagram of the Unfolded Graph (for $T=3$)}{56}{subsection.8.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces An RNN with 1 hidden layer, "unfolded" for $T=3$ time steps. This is now a deep feed-forward network with 3 layers. Critically, the weights for each connection type ($\mathbf  {W}_{xh}, \mathbf  {W}_{hh}, \mathbf  {W}_{hy}$) are \textbf  {shared} across all time steps.}}{57}{figure.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Ndtwork Training}{58}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Defining the Loss Function}{58}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}The Per-Time-Step Loss: $E^{(t)}$}{58}{subsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Case 1: Regression}{58}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Case 2: Classification}{58}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}The Total Loss: $E$}{58}{subsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Significance for Gradient Calculation}{59}{subsection.9.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Module 2.2: Deriving the Gradients (BPTT)}{59}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Case 1: Gradient for Output Weights ($\mathbf  {W}_{hy}$)}{59}{subsection.9.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces The gradient for $\mathbf  {W}_{hy}$ is local. It only depends on the error at time $t$ ($\frac  {\partial E^{(t)}}{\partial o^{(t)}}$) and the hidden state at time $t$ ($\mathbf  {h}^{(t)}$).}}{59}{figure.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Derivation (per time step $t$)}{59}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Gradient Calculation}{60}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Learn with example}{60}{section.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Part 1: The Problem Definition}{60}{subsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1.1: Architecture}{60}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1.2: Mathematical Equations}{60}{section*.67}\protected@file@percent }
\newlabel{eq:a_sub}{{9.8}{60}{1.2: Mathematical Equations}{equation.9.8}{}}
\newlabel{eq:h_sub}{{9.9}{60}{1.2: Mathematical Equations}{equation.9.9}{}}
\newlabel{eq:o_sub}{{9.10}{60}{1.2: Mathematical Equations}{equation.9.10}{}}
\newlabel{eq:y_sub}{{9.11}{60}{1.2: Mathematical Equations}{equation.9.11}{}}
\newlabel{eq:Et_sub}{{9.12}{60}{1.2: Mathematical Equations}{equation.9.12}{}}
\newlabel{eq:E_sub}{{9.13}{60}{1.2: Mathematical Equations}{equation.9.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{1.3: The Unfolded Computational Graph (T=2)}{60}{section*.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces The 2-2-1 RNN, unfolded for $T=2$. Note the parameter sharing: the \textit  {same} matrices $\mathbf  {W}_{xh}$, $\mathbf  {W}_{hh}$, and $\mathbf  {W}_{hy}$ are used at both time steps.}}{61}{figure.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Part 2: The Numerical Example}{61}{subsection.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2.1: Initial Values}{61}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Part 3: Forward Pass (Numerical Calculation)}{62}{subsection.9.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3.1: Time Step $t=1$}{62}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3.2: Time Step $t=2$}{62}{section*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3.3: Total Loss}{62}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Part 4: Backward Pass (BPTT Derivations \& Calculations)}{63}{subsection.9.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4.1: Gradient for Output Weights $\mathbf  {W}_{hy}$ and $\mathbf  {b}_y$}{63}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4.1.1: General Derivation (per time step t)}{63}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4.1.2: Numerical Calculation}{63}{section*.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4.2: Gradients for Hidden Weights $\mathbf  {W}_{hh}$, $\mathbf  {W}_{xh}$, $\mathbf  {b}_h$}{63}{section*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4.2.1: General Derivation of $\delta _h^{(t)}$}{64}{section*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4.2.2: Numerical Calculation of $\delta _h^{(t)}$}{64}{section*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4.2.3: Final Gradient Calculation (W, b)}{65}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Part 5: Weight Update (Example)}{66}{subsection.9.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Update for $\mathbf  {W}_{hh}$:}{66}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Update for $\mathbf  {W}_{hy}$:}{66}{section*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The Recursive Jacobian Product}{66}{section.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}The Jacobian of the State Transition}{66}{subsection.9.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.5}The Problem of Long-Range Dependencies}{67}{section.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}3.1 Vanishing Gradients}{67}{subsection.9.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The Vanishing Gradient Problem. The error signal from $E^{(t)}$ is repeatedly multiplied by a Jacobian matrix with a norm $< 1$, causing it to vanish to zero before it reaches $h^{(k)}$.}}{67}{figure.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}3.2 Exploding Gradients}{68}{subsection.9.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}3.3 Solution for Exploding Gradients: Gradient Clipping}{68}{subsection.9.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Gated RNNs}{69}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}The Gating Mechanism}{69}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The Problem: The Unstable Gradient Path}{69}{subsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}The Conceptual Solution: An Additive "Cell State"}{69}{subsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}The Gating Mechanism: A Learnable "Soft Switch"}{69}{subsection.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Definition of a Gate}{70}{section*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. The Gate Vector $\mathbf  {g}$:}{70}{section*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. The Gating Operation $\odot $:}{70}{section*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Numerical Example:}{70}{section*.85}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces The Gating Mechanism. The input $\mathbf  {x}$ is fed into a sigmoid function to create a gate vector $\mathbf  {g}$. This gate $\mathbf  {g}$ controls which elements of $\mathbf  {y}_{\text  {data}}$ are allowed to pass through via element-wise multiplication.}}{70}{figure.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}The Gated Gradient Superhighway}{70}{subsection.10.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient Analysis of the Gated Highway}{70}{section*.86}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Transformers}{72}{part.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Attention}{73}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Introduction to Attention}{73}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Key Takeaways from the Introduction}{73}{subsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Attention Coefficients}{73}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Constraints on Attention Weights}{74}{subsection.11.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix: Common Activation Functions}{75}{appendix*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Identity Function}{75}{section*.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Logistic Sigmoid}{75}{section*.90}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hyperbolic Tangent (tanh)}{75}{section*.91}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{75}{section*.92}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax}{76}{section*.94}\protected@file@percent }
\gdef \@abspage@last{77}
